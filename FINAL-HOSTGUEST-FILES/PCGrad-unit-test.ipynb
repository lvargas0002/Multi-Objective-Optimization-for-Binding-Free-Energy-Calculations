{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef4f09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss1 = 3.9446, loss2 = 4.6393\n",
      "Epoch 2: loss1 = 1.6433, loss2 = 3.6033\n",
      "Epoch 3: loss1 = 0.3916, loss2 = 2.7050\n",
      "Epoch 4: loss1 = 0.1011, loss2 = 1.8749\n",
      "Epoch 5: loss1 = 0.1288, loss2 = 1.2099\n",
      "Epoch 6: loss1 = 0.0677, loss2 = 0.8024\n",
      "Epoch 7: loss1 = 0.0291, loss2 = 0.4881\n",
      "Epoch 8: loss1 = 0.0506, loss2 = 0.3068\n",
      "Epoch 9: loss1 = 0.0306, loss2 = 0.2097\n",
      "Epoch 10: loss1 = 0.0186, loss2 = 0.1591\n",
      "Epoch 11: loss1 = 0.0175, loss2 = 0.1316\n",
      "Epoch 12: loss1 = 0.0207, loss2 = 0.1133\n",
      "Epoch 13: loss1 = 0.0160, loss2 = 0.0883\n",
      "Epoch 14: loss1 = 0.0109, loss2 = 0.0627\n",
      "Epoch 15: loss1 = 0.0081, loss2 = 0.0409\n",
      "Epoch 16: loss1 = 0.0063, loss2 = 0.0288\n",
      "Epoch 17: loss1 = 0.0055, loss2 = 0.0187\n",
      "Epoch 18: loss1 = 0.0049, loss2 = 0.0148\n",
      "Epoch 19: loss1 = 0.0043, loss2 = 0.0125\n",
      "Epoch 20: loss1 = 0.0039, loss2 = 0.0089\n",
      "Epoch 21: loss1 = 0.0036, loss2 = 0.0075\n",
      "Epoch 22: loss1 = 0.0030, loss2 = 0.0071\n",
      "Epoch 23: loss1 = 0.0025, loss2 = 0.0057\n",
      "Epoch 24: loss1 = 0.0025, loss2 = 0.0050\n",
      "Epoch 25: loss1 = 0.0027, loss2 = 0.0045\n",
      "Epoch 26: loss1 = 0.0025, loss2 = 0.0037\n",
      "Epoch 27: loss1 = 0.0022, loss2 = 0.0033\n",
      "Epoch 28: loss1 = 0.0021, loss2 = 0.0030\n",
      "Epoch 29: loss1 = 0.0022, loss2 = 0.0026\n",
      "Epoch 30: loss1 = 0.0021, loss2 = 0.0027\n"
     ]
    }
   ],
   "source": [
    "## Test new PCGrad Implementation for TF2.0\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Dummy data: 100 samples, 10 features\n",
    "X = np.random.randn(100, 10).astype(np.float32)\n",
    "\n",
    "# Task 1: predict y1 = sum of features\n",
    "# Task 2: predict y2 = product of first two features\n",
    "y1 = np.sum(X, axis=1, keepdims=True).astype(np.float32)\n",
    "y2 = (X[:, 0] * X[:, 1]).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# Simple MLP model with one shared layer and two heads\n",
    "class MultiTaskModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.head1 = tf.keras.layers.Dense(1)  # for y1\n",
    "        self.head2 = tf.keras.layers.Dense(1)  # for y2\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.shared(inputs)\n",
    "        return self.head1(x), self.head2(x)\n",
    "\n",
    "model = MultiTaskModel()\n",
    "\n",
    "# Your working PCGrad class (assumed to be fixed as discussed)\n",
    "class PCGrad(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, optimizer, name=\"PCGrad\", **kwargs):\n",
    "        super().__init__(name=name, learning_rate=0.0, **kwargs)\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, name=None, **kwargs):\n",
    "        return self._optimizer.apply_gradients(grads_and_vars, **kwargs)\n",
    "\n",
    "    def compute_gradients(self, losses, tape, var_list):\n",
    "        assert isinstance(losses, list)\n",
    "        grads_task = []\n",
    "        for loss in losses:\n",
    "            grads = tape.gradient(loss, var_list)\n",
    "            grads = [tf.zeros_like(v) if g is None else g for g, v in zip(grads, var_list)]\n",
    "            grads_task.append(grads)\n",
    "\n",
    "        def flatten(grads):\n",
    "            return tf.concat([tf.reshape(g, [-1]) for g in grads], axis=0)\n",
    "\n",
    "        flat_grads_task = tf.stack([flatten(g) for g in grads_task])\n",
    "        flat_grads_task = tf.random.shuffle(flat_grads_task)\n",
    "\n",
    "        def project(g, others):\n",
    "            for o in others:\n",
    "                dot = tf.reduce_sum(g * o)\n",
    "                g -= tf.cond(dot < 0, lambda: dot / (tf.reduce_sum(o * o) + 1e-12) * o, lambda: tf.zeros_like(g))\n",
    "            return g\n",
    "\n",
    "        projected = [project(g, tf.concat([flat_grads_task[:i], flat_grads_task[i+1:]], axis=0))\n",
    "                     for i, g in enumerate(flat_grads_task)]\n",
    "        mean_grad = tf.reduce_mean(tf.stack(projected), axis=0)\n",
    "\n",
    "        reshaped_grads = []\n",
    "        idx = 0\n",
    "        for v in var_list:\n",
    "            shape = tf.shape(v)\n",
    "            size = tf.reduce_prod(shape)\n",
    "            reshaped_grads.append(tf.reshape(mean_grad[idx:idx + size], shape))\n",
    "            idx += size\n",
    "\n",
    "        return list(zip(reshaped_grads, var_list))\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._optimizer.learning_rate\n",
    "\n",
    "# Optimizer\n",
    "opt = PCGrad(tf.keras.optimizers.Adam(1e-2))\n",
    "\n",
    "# Loss functions\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y1_batch = y1[i:i+batch_size]\n",
    "        y2_batch = y2[i:i+batch_size]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            out1, out2 = model(X_batch)\n",
    "            loss1 = mse(y1_batch, out1)\n",
    "            loss2 = mse(y2_batch, out2)\n",
    "\n",
    "        grads_and_vars = opt.compute_gradients([loss1, loss2], tape, model.trainable_variables)\n",
    "        opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss1 = {loss1.numpy():.4f}, loss2 = {loss2.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644faee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ligand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
