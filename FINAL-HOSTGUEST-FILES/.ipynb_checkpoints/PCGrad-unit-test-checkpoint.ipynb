{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef4f09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss1 = 5.8065, loss2 = 0.0367\n",
      "Epoch 2: loss1 = 1.9382, loss2 = 0.0920\n",
      "Epoch 3: loss1 = 0.1909, loss2 = 0.0428\n",
      "Epoch 4: loss1 = 0.2313, loss2 = 0.0336\n",
      "Epoch 5: loss1 = 0.3556, loss2 = 0.0267\n",
      "Epoch 6: loss1 = 0.1070, loss2 = 0.0320\n",
      "Epoch 7: loss1 = 0.0486, loss2 = 0.0244\n",
      "Epoch 8: loss1 = 0.0830, loss2 = 0.0082\n",
      "Epoch 9: loss1 = 0.0340, loss2 = 0.0037\n",
      "Epoch 10: loss1 = 0.0057, loss2 = 0.0051\n",
      "Epoch 11: loss1 = 0.0054, loss2 = 0.0052\n",
      "Epoch 12: loss1 = 0.0113, loss2 = 0.0045\n",
      "Epoch 13: loss1 = 0.0135, loss2 = 0.0037\n",
      "Epoch 14: loss1 = 0.0101, loss2 = 0.0033\n",
      "Epoch 15: loss1 = 0.0071, loss2 = 0.0029\n",
      "Epoch 16: loss1 = 0.0057, loss2 = 0.0022\n",
      "Epoch 17: loss1 = 0.0046, loss2 = 0.0027\n",
      "Epoch 18: loss1 = 0.0037, loss2 = 0.0026\n",
      "Epoch 19: loss1 = 0.0028, loss2 = 0.0016\n",
      "Epoch 20: loss1 = 0.0018, loss2 = 0.0015\n",
      "Epoch 21: loss1 = 0.0012, loss2 = 0.0017\n",
      "Epoch 22: loss1 = 0.0010, loss2 = 0.0014\n",
      "Epoch 23: loss1 = 0.0008, loss2 = 0.0011\n",
      "Epoch 24: loss1 = 0.0005, loss2 = 0.0012\n",
      "Epoch 25: loss1 = 0.0004, loss2 = 0.0011\n",
      "Epoch 26: loss1 = 0.0004, loss2 = 0.0008\n",
      "Epoch 27: loss1 = 0.0003, loss2 = 0.0007\n",
      "Epoch 28: loss1 = 0.0002, loss2 = 0.0007\n",
      "Epoch 29: loss1 = 0.0002, loss2 = 0.0008\n",
      "Epoch 30: loss1 = 0.0001, loss2 = 0.0007\n"
     ]
    }
   ],
   "source": [
    "## Test new PCGrad Implementation for TF2.0\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Dummy data: 100 samples, 10 features\n",
    "X = np.random.randn(100, 10).astype(np.float32)\n",
    "\n",
    "# Task 1: predict y1 = sum of features\n",
    "# Task 2: predict y2 = product of first two features\n",
    "y1 = np.sum(X, axis=1, keepdims=True).astype(np.float32)\n",
    "y2 = (X[:, 0] * X[:, 1]).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# Simple MLP model with one shared layer and two heads\n",
    "class MultiTaskModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.head1 = tf.keras.layers.Dense(1)  # for y1\n",
    "        self.head2 = tf.keras.layers.Dense(1)  # for y2\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.shared(inputs)\n",
    "        return self.head1(x), self.head2(x)\n",
    "\n",
    "model = MultiTaskModel()\n",
    "\n",
    "# Your working PCGrad class (assumed to be fixed as discussed)\n",
    "# TypeError: learning_rate is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.\n",
    "# I removed learning_rate =0.0\n",
    "class PCGrad(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, optimizer, name=\"PCGrad\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, name=None, **kwargs):\n",
    "        return self._optimizer.apply_gradients(grads_and_vars, **kwargs)\n",
    "\n",
    "    def compute_gradients(self, losses, tape, var_list):\n",
    "        assert isinstance(losses, list)\n",
    "        grads_task = []\n",
    "        for loss in losses:\n",
    "            grads = tape.gradient(loss, var_list)\n",
    "            grads = [tf.zeros_like(v) if g is None else g for g, v in zip(grads, var_list)]\n",
    "            grads_task.append(grads)\n",
    "\n",
    "        def flatten(grads):\n",
    "            return tf.concat([tf.reshape(g, [-1]) for g in grads], axis=0)\n",
    "\n",
    "        flat_grads_task = tf.stack([flatten(g) for g in grads_task])\n",
    "        flat_grads_task = tf.random.shuffle(flat_grads_task)\n",
    "\n",
    "        def project(g, others):\n",
    "            for o in others:\n",
    "                dot = tf.reduce_sum(g * o)\n",
    "                g -= tf.cond(dot < 0, lambda: dot / (tf.reduce_sum(o * o) + 1e-12) * o, lambda: tf.zeros_like(g))\n",
    "            return g\n",
    "\n",
    "        projected = [project(g, tf.concat([flat_grads_task[:i], flat_grads_task[i+1:]], axis=0))\n",
    "                     for i, g in enumerate(flat_grads_task)]\n",
    "        mean_grad = tf.reduce_mean(tf.stack(projected), axis=0)\n",
    "\n",
    "        reshaped_grads = []\n",
    "        idx = 0\n",
    "        for v in var_list:\n",
    "            shape = tf.shape(v)\n",
    "            size = tf.reduce_prod(shape)\n",
    "            reshaped_grads.append(tf.reshape(mean_grad[idx:idx + size], shape))\n",
    "            idx += size\n",
    "\n",
    "        return list(zip(reshaped_grads, var_list))\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._optimizer.learning_rate\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "opt = PCGrad(tf.keras.optimizers.Adam(1e-2))\n",
    "\n",
    "# Loss functions\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y1_batch = y1[i:i+batch_size]\n",
    "        y2_batch = y2[i:i+batch_size]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            out1, out2 = model(X_batch)\n",
    "            loss1 = mse(y1_batch, out1)\n",
    "            loss2 = mse(y2_batch, out2)\n",
    "\n",
    "        grads_and_vars = opt.compute_gradients([loss1, loss2], tape, model.trainable_variables)\n",
    "        opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss1 = {loss1.numpy():.4f}, loss2 = {loss2.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644faee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf69cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
